<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>TJARK-OPL RoboCup Team</title>
<style>
    body {font-family: Arial, sans-serif; margin:0; padding:0; line-height:1.6;}
    header {background:#004080; color:white; padding:20px 0; text-align:center;}
    nav {text-align:center; margin:10px 0;}
    nav a {margin:0 15px; color:#004080; text-decoration:none; font-weight:bold;}
    nav a:hover {text-decoration:underline;}
    section {padding:50px 20px; max-width:1000px; margin:0 auto;}
    h2 {border-bottom:2px solid #004080; padding-bottom:5px;}
    img {max-width:100%; height:auto; margin:10px 0;}
    iframe {max-width:100%; height:400px;}
    .team-member {display:flex; flex-wrap:wrap; gap:20px;}
    .member {flex:1 1 200px; text-align:center;}
    #team table {border-collapse: collapse; width: 100%;}
    #team table, #team th, #team td {border: 1px solid #000;}
    #team th, #team td {padding: 8px; text-align: left;}
    #instructors table {border-collapse: collapse; width: 100%;}
    #instructors table, #instructors th, #instructors td {border: 1px solid #000;}
    #instructors th, #instructors td {padding: 8px; text-align: left;}
</style>
</head>
<body>

<header>
    <h1>TJARK-OPL RoboCup Team</h1>
    <nav>
        <a href="#overview">Home</a>
        <a href="#video">Media</a>
        <a href="#publications">Publications</a>
        <a href="#team">Team</a>
        <a href="#instructors">Instructors</a>
        <a href="#history">History</a>
    </nav>
</header>

<section id="overview">
    <h2>Introduction</h2>

    <!-- RoboCup@Home League -->
    <h3>The RoboCup@Home League</h3>
    <p>
        The RoboCup@Home league aims to develop service and assistive robot technology with high relevance for future personal domestic applications. 
        It is the largest international annual competition for autonomous service robots and is part of the RoboCup initiative. 
        A set of benchmark tests is used to evaluate the robots’ abilities and performance in a realistic non-standardized home environment setting. 
        Focus lies on the following domains but is not limited to: Human-Robot Interaction and Cooperation, Navigation and Mapping in dynamic environments, 
        Computer Vision and Object Recognition under natural light conditions, Object Manipulation, Adaptive Behaviors, Behavior Integration, 
        Ambient Intelligence, Standardization, and System Integration. It is colocated with the RoboCup symposium.
    </p>

    <!-- TJArk@Home Overview -->
    <h3>TJArk@Home</h3>
    <p>
        The Robotics and Artificial Intelligence Lab (RAIL) of Tongji University was founded in 1992. 
        Team TJArk of RAIL was founded in 2004 and participated in the RoboCup World Cup from 2006 to 2019. 
        We achieved seven consecutive championships in the China RoboCup SPL and once won third place in the RoboCup 2018 SPL.
    </p>
    <p>
        In December 2018, we founded a new energetic team to participate in the RoboCup@Home League, called TJArk@Home. 
        The main goal of TJArk@Home is to explore how well robots can serve people in daily life at an acceptable cost. 
        Guided by this vision, we conduct extensive research on our robot platform, including:
    </p>
    <ul>
        <li>SLAM (Simultaneous Localization and Mapping)</li>
        <li>Autonomous navigation</li>
        <li>Object detection and recognition</li>
        <li>Human-face detection and analysis</li>
        <li>Human–robot interaction</li>
        <li>Motion control</li>
        <li>Trajectory teaching</li>
        <li>Design of laptop-based GUI</li>
    </ul>
    <p>
        In April 2019, TJArk@Home participated in the China RoboCup@Home League for the first time and earned first place in the SSPL (Social Standard Platform League). 
        During the competition, we demonstrated various abilities, including autonomous navigation, following an operator to a given position, 
        speech recognition and response, and human detection with feature summarization.
    </p>

    <!-- Visual SLAM -->
    <h3>Visual Simultaneous Localization and Mapping</h3>
    <p>
        Our robot requires an environmental model (map) to support tasks such as autonomous navigation. 
        However, prior maps are often unavailable in dynamic home scenarios, making SLAM essential. 
        Stereo camera data is used by a visual SLAM module, assisted by limited laser data and odometry inputs. 
        Our VSLAM algorithm is based on RTAB-Map and modified for our robot’s sensors. 
        Odometry and laser data provide initial pose estimation, while stereo data performs appearance-based loop closure to optimize local and global poses.
    </p>
    <p>
        The VSLAM system generates a 3D point cloud, which we project into a 2D grid map for navigation. 
        With the map and robot parameters, the DWA (Dynamic Window Approach) algorithm plans a safe path. 
        Vision, laser, and sonar data are fused for real-time obstacle avoidance in dynamic environments. 
        Motion commands are then sent through robot APIs to enable fully autonomous navigation.
    </p>

    <!-- Computer Vision -->
    <h3>Computer Vision</h3>
    <p>
        Various vision algorithms are applied to support perception of objects and humans. 
        Our robot can detect and recognize different people, remember names and features, and track them for interactive tasks. 
        For object-level reasoning, we designed a detection framework based on YOLO for real-time indoor object recognition across 50 categories. 
        Training data is collected from large public datasets, and adjacent frame matching is used to improve robustness.
    </p>
    <p>
        As the robot’s built-in camera has a limited field of view, we expand visual perception by stitching images from multiple viewpoints, 
        followed by fusion algorithms to reduce lighting and geometric inconsistencies. 
        This enables more complete visual information for high-level processing.
    </p>
    <p>
        For human pose estimation, we employ OpenPose to achieve robust, real-time multi-person body-part detection using a non-parametric representation.
    </p>

    <!-- Software Framework -->
    <h3>Software Framework</h3>
    <p>
        We developed a Python-based framework that integrates NAOqi APIs and ROS components. 
        Team members can easily add custom modules (e.g., motion, detection, navigation) to the framework. 
        During competitions, the robot can execute complex tasks using these modules. 
        The framework is still under development, so it is not open-sourced yet.
    </p>
</section>

<section id="video">
    <h2>Media</h2>
    <h3>Photos</h3>
    <div class="photo-gallery">
        <img src="assets/photos/robot1.png" alt="robot1">
        <img src="assets/photos/robot2.png" alt="robot2">
    </div>
    
    <style>
        .photo-gallery {
            display: flex;
            flex-wrap: wrap;
            gap: 15px;
        }
        .photo-gallery img {
            width: 32%;
            border-radius: 8px;
        }
    </style>
    
    <h3>Qualification Video</h3>
    <iframe
        src="https://www.youtube.com/embed/Mq5DDaTxdFg"
        frameborder="0"
        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
        allowfullscreen
        title="TJARK-OPL">
    </iframe>
</section>
    
<section id="publications">
    <h2>Publications</h2>
    <ul>
        <li><a href="https://ieeexplore.ieee.org/document/10004992" target="_blank">L. Wang, Z. He, R. Dang, H. Chen, C. Liu, and Q. Chen, “RES-StS: Referring expression speaker via self-training with scorer for goal-oriented vision-language navigation,” IEEE Transactions on Circuits and Systems for Video Technology, 2023, doi: 10.1109/TCSVT.2022.3233554</a></li>
        <li><a href="https://www.ijcai.org/proceedings/2023/0164.pdf" target="_blank">L. Wang, Z. He, J. Tang, R. Dang, N. Wang, C. Liu, and Q. Chen, “A dual semantic-aware recurrent global-adaptive network for vision-and-language navigation,” in Proceedings of the thirty-second international joint conference on artificial intelligence, IJCAI-23, E. Elkind, Ed., International Joint Conferences on Artificial Intelligence Organization, Aug. 2023, pp. 1479–1487. doi: 10.24963/ijcai.2023/164</a></li>
        <li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0952197623016718" target="_blank">L. Wang, C. Liu, Z. He, S. Li, Q. Yan, H. Chen, and Q. Chen, “PASTS: Progress-aware spatio-temporal transformer speaker for vision-and-language navigation,” Engineering Applications of Artificial Intelligence, 2023, doi: https://doi.org/10.1016/j.engappai.2023.107487</a></li>
        <li><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Vision-and-Language_Navigation_via_Causal_Learning_CVPR_2024_paper.pdf" target="_blank">L. Wang, Z. He, R. Dang, M. Shen, C. Liu, and Q. Chen, “Vision-and-language navigation via causal learning,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), Jun. 2024, pp. 13139–13150. doi: 10.1109/CVPR52733.2024.01248</a></li>
        <li><a href="https://ieeexplore.ieee.org/document/11115085" target="_blank">Z. He, L. Wang, L. Chen, C. Liu and Q. Chen, "NavComposer: Composing Language Instructions for Navigation Trajectories through Action-Scene-Object Modularization," in IEEE Transactions on Circuits and Systems for Video Technology, doi: 10.1109/TCSVT.2025.3596386</a></li>
        <li><a href="https://ieeexplore.ieee.org/document/10802484/" target="_blank">Z. He, L. Wang, L. Chen, S. Li, Q. Yan, C. Liu, and Q. Chen, “Multimodal evolutionary encoder for continuous vision-language navigation,” in 2024 IEEE/RSJ international conference on intelligent robots and systems (IROS), Oct. 2024, pp. 1443–1450. doi: 10.1109/IROS58592.2024.10802484</a></li>
        <li><a href="https://ieeexplore.ieee.org/document/10288539" target="_blank">Z. He, L. Wang, R. Dang, S. Li, Q. Yan, C. Liu, and Q. Chen, “Learning depth representation from RGB-d videos by time-aware contrastive pre-training,” IEEE Transactions on Circuits and Systems for Video Technology, vol. 34, no. 6, pp. 4143–4158, Jun. 2024, doi: 10.1109/TCSVT.2023.3326373</a></li>
    </ul>
</section>
    
<section id="team">
    <h2>Team Members</h2>
    <table>
        <thead>
            <tr><th>Name</th><th>Introduction</th></tr>
        </thead>
        <tbody>
            <tr><td>Yang Xu</td><td>Leader, Graduate student, Department of Control Science and Engineering</td></tr>
            <tr><td>Zhu Qijia</td><td>Graduate student, Department of Control Science and Engineering</td></tr>
            <tr><td>Qian Liang</td><td>Graduate student, Department of Control Science and Engineering</td></tr>
            <tr><td>Chen Yuzhe</td><td>Graduate student, Department of Control Science and Engineering</td></tr>
            <tr><td>Sheng Kai</td><td>Senior student, Artificial Intelligence</td></tr>
            <tr><td>Zhu Siya</td><td>Senior student, Mathematics</td></tr>
            <tr><td>Wang Zhaofengnian</td><td>Senior student, Physics</td></tr>
            <tr><td>Wang Xiangyi</td><td>Junior student, Artificial Intelligence</td></tr>
            <tr><td>Qi Chengkai</td><td>Junior student, Artificial Intelligence</td></tr>
            <tr><td>Li Yingxuan</td><td>Sophomore student, Robotics</td></tr>
        </tbody>
    </table>
</section>

<section id="instructors">
    <h2>Instructors</h2>
    <table>
        <thead>
            <tr><th>Name</th><th>Introduction</th></tr>
        </thead>
        <tbody>
            <tr><td>Chen Qijun</td><td>Professor, Department of Control Science and Engineering</td></tr>
            <tr><td>Liu Chengju</td><td>Professor, Department of Control Science and Engineering</td></tr>
            <tr><td>Yao Chenpeng</td><td>Assistant Professor, Department of Control Science and Engineering</td></tr>
        </tbody>
    </table>
</section>

<section id="history">
    <h2>RoboCup Participation</h2>
    <p>Previous participation and rankings in RoboCup and local tournaments.</p>
    <ul>
        <li>2019 RoboCup SSPL – Champion</li>
        <li>2020 RoboCup SSPL – Champion</li>
        <li>2021 RoboCup SSPL – Champion</li>
        <li>2021 RoboCup OPL – Champion</li>
        <li>2022 RoboCup SSPL – First Prize</li>
        <li>2025 RoboCup OPL – Champion</li>
    </ul>
</section>

</body>
</html>
