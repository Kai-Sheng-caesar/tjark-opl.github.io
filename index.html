<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>TJArk-OPL RoboCup Team</title>

<!-- Google Font -->
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">

<style>
:root {
    --primary: #004080;
    --bg: #f7f9fc;
    --card: #ffffff;
    --text: #222;
}

* {
    box-sizing: border-box;
}

body {
    margin: 0;
    font-family: "Inter", "Segoe UI", Arial, sans-serif;
    background: var(--bg);
    color: var(--text);
    line-height: 1.7;
}

/* Header */
header {
    background: linear-gradient(135deg, #003366, #0055aa);
    color: white;
    padding: 60px 20px;
    text-align: center;
}

header h1 {
    margin-bottom: 10px;
    font-size: 2.6rem;
}

nav a {
    color: white;
    margin: 0 15px;
    text-decoration: none;
    font-weight: 600;
    position: relative;
}

nav a::after {
    content: "";
    position: absolute;
    left: 0;
    bottom: -6px;
    width: 0;
    height: 2px;
    background: white;
    transition: width 0.3s;
}

nav a:hover::after {
    width: 100%;
}

/* Section Card */
section {
    max-width: 1100px;
    margin: 50px auto;
    background: var(--card);
    padding: 40px;
    border-radius: 14px;
    box-shadow: 0 10px 30px rgba(0,0,0,0.06);
}

h2 {
    color: var(--primary);
    margin-bottom: 20px;
    border-left: 6px solid var(--primary);
    padding-left: 10px;
}

h3 {
    margin-top: 30px;
}

/* Lists */
ul {
    margin-left: 20px;
}

/* Gallery */
.gallery {
    column-count: 3;
    column-gap: 15px;
}

.gallery img {
    width: 100%;
    margin-bottom: 15px;
    border-radius: 10px;
    break-inside: avoid;
    cursor: pointer;
    transition: transform 0.4s ease, box-shadow 0.4s ease;
}

.gallery img:hover {
    transform: scale(1.03);
    box-shadow: 0 15px 30px rgba(0,0,0,0.25);
}

/* Video */
iframe {
    width: 100%;
    height: 420px;
    border-radius: 12px;
    margin-top: 20px;
}

/* Tables */
table {
    width: 100%;
    border-collapse: collapse;
    margin-top: 20px;
}

th, td {
    padding: 12px;
    border-bottom: 1px solid #ddd;
}

th {
    background: #f1f4f9;
    text-align: left;
}

/* History */
#history ul li {
    margin: 8px 0;
}

/* Responsive */
@media (max-width: 900px) {
    .gallery {
        column-count: 2;
    }
}

@media (max-width: 600px) {
    .gallery {
        column-count: 1;
    }
}
</style>
</head>

<body>

<header>
    <h1>TJark-OPL RoboCup Team</h1>
    <nav>
        <a href="#overview">Home</a>
        <a href="#media">Media</a>
        <a href="#publications">Publications</a>
        <a href="#instructors">Instructors</a>
        <a href="#team">Team</a>
        <a href="#history">History</a>
    </nav>
</header>

<style>
header {
    position: relative;
    color: white;
    text-align: center;
    padding: 80px 20px;
    background: url('assets/photos/head.jpg') no-repeat center center;
    background-size: cover;
    overflow: hidden;
}

header::after {
    content: "";
    position: absolute;
    top: 0; left: 0;
    width: 100%;
    height: 100%;
    background: rgba(0,0,0,0.4); 
    z-index: 0;
}

header h1 {
    position: relative;
    font-size: 2.8rem;
    margin-bottom: 15px;
    z-index: 1; 
}

header nav {
    position: relative;
    z-index: 1;
}

header nav a {
    color: white;
    margin: 0 15px;
    text-decoration: none;
    font-weight: 600;
    position: relative;
}

header nav a::after {
    content: "";
    position: absolute;
    left: 0;
    bottom: -6px;
    width: 0;
    height: 2px;
    background: white;
    transition: width 0.3s;
}

header nav a:hover::after {
    width: 100%;
}
</style>

<section id="overview">
    <h2>Introduction</h2>

    <!-- RoboCup@Home League -->
    <h3>The RoboCup@Home League</h3>
    <p>
        The RoboCup@Home league aims to develop service and assistive robot technology with high relevance for future personal domestic applications. 
        It is the largest international annual competition for autonomous service robots and is part of the RoboCup initiative. 
        A set of benchmark tests is used to evaluate the robots’ abilities and performance in a realistic non-standardized home environment setting. 
        Focus lies on the following domains but is not limited to: Human-Robot Interaction and Cooperation, Navigation and Mapping in dynamic environments, 
        Computer Vision and Object Recognition under natural light conditions, Object Manipulation, Adaptive Behaviors, Behavior Integration, 
        Ambient Intelligence, Standardization, and System Integration. It is colocated with the RoboCup symposium.
    </p>

    <!-- TJArk@Home Overview -->
    <h3>TJArk@Home</h3>
    <p>
        The Robotics and Artificial Intelligence Lab (RAIL) of Tongji University was founded in 1992. 
        Team TJArk of RAIL was founded in 2004 and participated in the RoboCup World Cup from 2006 to 2019. 
        We achieved seven consecutive championships in the China RoboCup SPL and once won third place in the RoboCup 2018 SPL.
    </p>
    <p>
        In December 2018, we founded a new energetic team to participate in the RoboCup@Home League, called TJArk@Home. 
        The main goal of TJArk@Home is to explore how well robots can serve people in daily life at an acceptable cost. 
        Guided by this vision, we conduct extensive research on our robot platform, including:
    </p>
    <ul>
        <li>SLAM (Simultaneous Localization and Mapping)</li>
        <li>Autonomous navigation</li>
        <li>Object detection and recognition</li>
        <li>Human-face detection and analysis</li>
        <li>Human–robot interaction</li>
        <li>Motion control</li>
        <li>Trajectory teaching</li>
    </ul>
    <p>
        In April 2019, TJArk@Home participated in the China RoboCup@Home League for the first time and earned first place in the SSPL (Social Standard Platform League). 
        During the competition, we demonstrated various abilities, including autonomous navigation, following an operator to a given position, 
        speech recognition and response, and human detection with feature summarization.
    </p>

    <!-- Visual SLAM -->
    <h3>Visual Simultaneous Localization and Mapping</h3>
    <p>
        Our robot requires an environmental model (map) to support tasks such as autonomous navigation. 
        However, prior maps are often unavailable in dynamic home scenarios, making SLAM essential. 
        Stereo camera data is used by a visual SLAM module, assisted by limited laser data and odometry inputs. 
        Our VSLAM algorithm is based on RTAB-Map and modified for our robot’s sensors. 
        Odometry and laser data provide initial pose estimation, while stereo data performs appearance-based loop closure to optimize local and global poses.
    </p>

    <div class="obj-gallery">
        <img src="assets/photos/slam.png" alt="SLAM Map 1">
        <img src="assets/photos/slam2.png" alt="SLAM Map 2">
    </div>
    
    <p>
        The VSLAM system generates a 3D point cloud, which we project into a 2D grid map for navigation. 
        With the map and robot parameters, the DWA (Dynamic Window Approach) algorithm plans a safe path. 
        Vision, laser, and sonar data are fused for real-time obstacle avoidance in dynamic environments. 
        Motion commands are then sent through robot APIs to enable fully autonomous navigation.
    </p>

    <!-- Computer Vision -->
    <h3>Computer Vision</h3>
    <p>
        Various vision algorithms are applied to support perception of objects and humans. 
        Our robot can detect and recognize different people, remember names and features, and track them for interactive tasks. 
        For object-level reasoning, we designed a detection framework based on YOLO for real-time indoor object recognition across 50 categories. 
        Training data is collected from large public datasets, and adjacent frame matching is used to improve robustness.
    </p>
    
    <div class="obj-gallery">
        <img src="assets/photos/obj_detect.jpg" alt="Object Detection 1">
        <img src="assets/photos/obj2.png" alt="Object Detection 2">
        <img src="assets/photos/obj3.jpg" alt="Object Detection 3">
    </div>
    
    <p>
        As the robot’s built-in camera has a limited field of view, we expand visual perception by stitching images from multiple viewpoints, 
        followed by fusion algorithms to reduce lighting and geometric inconsistencies. 
        This enables more complete visual information for high-level processing.
    </p>
    <p>
        For human pose estimation, we employ OpenPose to achieve robust, real-time multi-person body-part detection using a non-parametric representation.
    </p>
    <style>
    .obj-gallery {
        display: flex;
        gap: 15px;
        justify-content: center;
        flex-wrap: wrap;
        margin: 20px 0;
    }
    
    .obj-gallery img {
        width: 32%;
        border-radius: 8px;
        object-fit: cover;
        cursor: pointer;
        transition: transform 0.4s ease, box-shadow 0.4s ease;
    }
    
    .obj-gallery img:hover {
        transform: scale(1.03);
        box-shadow: 0 15px 30px rgba(0,0,0,0.25);
    }
    
    @media (max-width: 900px) {
        .obj-gallery img {
            width: 48%;
        }
    }
    
    @media (max-width: 600px) {
        .obj-gallery img {
            width: 100%;
        }
    }
    </style>

    <!-- Software Framework -->
    <h3>Software Framework</h3>
    <p>
        We developed a Python-based framework that integrates ROS components. 
        Team members can easily add custom modules (e.g., motion, detection, navigation) to the framework. 
        During competitions, the robot can execute complex tasks using these modules. 
        The framework is still under development, so it is not open-sourced yet.
    </p>
</section>

<section id="media">
    <h2>Media</h2>

    <h3>Photos</h3>
    <div class="slideshow-container">
        <div class="slide"><img src="assets/photos/robot1.png" alt="robot1"></div>
        <div class="slide"><img src="assets/photos/robot2.png" alt="robot2"></div>
        <div class="slide"><img src="assets/photos/robot3.jpg" alt="robot3"></div>
        <div class="slide"><img src="assets/photos/robot4.jpg" alt="robot4"></div>
        <div class="slide"><img src="assets/photos/robot5.jpg" alt="robot5"></div>
        <div class="slide"><img src="assets/photos/robot6.jpg" alt="robot6"></div>

        <a class="prev">&#10094;</a>
        <a class="next">&#10095;</a>
    </div>

    <h3>Qualification Video</h3>

    <p style="margin-top: 10px; font-size: 1.05rem;">
    This qualification video demonstrates our robot’s performance in competition and real-world testing scenarios.
    It includes a live demonstration of the GPSR task at the 2025 RoboCup OPL (China),
    followed by an experimental showcase of mobile manipulation with a robotic arm.
    </p>
    
    <p style="margin-top: 15px; font-size: 0.95rem;">
    <strong>Video chapters:</strong><br>
    <a href="https://www.youtube.com/watch?v=Mq5DDaTxdFg&t=0s" target="_blank">
        0:00 – 2:40 | GPSR task demonstration at 2025 RoboCup OPL (China)
    </a><br>
    <a href="https://www.youtube.com/watch?v=Mq5DDaTxdFg&t=160s" target="_blank">
        2:40 – 4:49 | Mobile manipulation and arm grasping test in experimental environment
    </a>
    </p>

    <div class="youtube-container">
        <iframe 
            src="https://www.youtube.com/embed/Mq5DDaTxdFg"
            frameborder="0"
            allowfullscreen>
        </iframe>
    </div>
    
    <p class="video-link">
        <strong>Full video link:</strong>
        <a href="https://www.youtube.com/watch?v=Mq5DDaTxdFg" target="_blank">
            https://www.youtube.com/watch?v=Mq5DDaTxdFg
        </a>
    </p>
</section>

<style>
.slideshow-container {
    position: relative;
    max-width: 100%;
    height: 450px; 
    margin: 20px auto;
    overflow: hidden;
    border-radius: 12px;
    background: rgba(0, 0, 0, 0.2); 
}

.slide {
    position: absolute;
    width: 100%;
    height: 100%;
    display: none;
}

.slide img {
    width: 100%;
    height: 100%;
    object-fit: contain; 
}

.prev, .next {
    cursor: pointer;
    position: absolute;
    top: 50%;
    width: auto;
    padding: 12px;
    margin-top: -22px;
    color: white;
    font-weight: bold;
    font-size: 28px;
    user-select: none;
    border-radius: 50%;
    background: rgba(0,0,0,0.4); 
    transition: 0.3s;
    z-index: 2;
}

.prev:hover, .next:hover {
    background: rgba(0,0,0,0.7);
}

.prev { left: 10px; }
.next { right: 10px; }

.video-container video {
    width: 100%;
    max-height: 450px;
    border-radius: 12px;
    background: black;
    box-shadow: 0 10px 30px rgba(0,0,0,0.15);
}

.youtube-container {
    position: relative;
    width: 100%;
    padding-top: 56.25%; 
    margin-bottom: 12px;
}

.youtube-container iframe {
    position: absolute;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
}

.video-link {
    margin-top: 36px;
    font-size: 0.95rem;
}
</style>

<script>
let slideIndex = 0;
const slides = document.querySelectorAll('.slide');
const totalSlides = slides.length;

function showSlides(n) {
    slides.forEach(slide => slide.style.display = 'none');
    slideIndex = (n + totalSlides) % totalSlides;
    slides[slideIndex].style.display = 'block';
}

function autoSlides() {
    slideIndex++;
    showSlides(slideIndex);
    setTimeout(autoSlides, 6000); 
}

document.querySelector('.prev').addEventListener('click', () => {
    showSlides(slideIndex - 1);
});
document.querySelector('.next').addEventListener('click', () => {
    showSlides(slideIndex + 1);
});

showSlides(slideIndex);
autoSlides();
</script>

<section id="publications">
    <h2>Publications</h2>
    <ul>
        <li><a href="https://ieeexplore.ieee.org/document/10004992" target="_blank">L. Wang, Z. He, R. Dang, H. Chen, C. Liu, and Q. Chen, “RES-StS: Referring expression speaker via self-training with scorer for goal-oriented vision-language navigation,” IEEE TCSVT, 2023</a></li>
        <li><a href="https://www.ijcai.org/proceedings/2023/0164.pdf" target="_blank">L. Wang, Z. He, J. Tang, R. Dang, et al., “A dual semantic-aware recurrent global-adaptive network for vision-and-language navigation,” IJCAI-23, 2023</a></li>
        <li><a href="https://www.sciencedirect.com/science/article/abs/pii/S0952197623016718" target="_blank">L. Wang, C. Liu, Z. He, et al., “PASTS: Progress-aware spatio-temporal transformer speaker for vision-and-language navigation,” Eng. Appl. AI, 2023</a></li>
        <li><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Wang_Vision-and-Language_Navigation_via_Causal_Learning_CVPR_2024_paper.pdf" target="_blank">L. Wang, Z. He, R. Dang, M. Shen, et al., “Vision-and-language navigation via causal learning,” CVPR 2024</a></li>
        <li><a href="https://ieeexplore.ieee.org/document/11115085" target="_blank">Z. He, L. Wang, L. Chen, et al., "NavComposer: Composing Language Instructions for Navigation Trajectories," IEEE TCSVT, 2025</a></li>
        <li><a href="https://ieeexplore.ieee.org/document/10802484/" target="_blank">Z. He, L. Wang, L. Chen, S. Li, et al., “Multimodal evolutionary encoder for continuous vision-language navigation,” IROS 2024</a></li>
        <li><a href="https://ieeexplore.ieee.org/document/10288539" target="_blank">Z. He, L. Wang, R. Dang, S. Li, et al., “Learning depth representation from RGB-d videos by time-aware contrastive pre-training,” IEEE TCSVT, 2024</a></li>
    </ul>
</section>

<section id="instructors">
    <h2>Instructors</h2>
    <table>
        <thead>
            <tr><th>Name</th><th>Introduction</th></tr>
        </thead>
        <tbody>
            <tr><td>Chen Qijun</td><td>Professor, Department of Control Science and Engineering, Tongji University</td></tr>
            <tr><td>Liu Chengju</td><td>Professor, Department of Control Science and Engineering, Tongji University</td></tr>
            <tr><td>Yao Chenpeng</td><td>Assistant Professor, Department of Control Science and Engineering, Tongji University</td></tr>
            <tr><td>Yan Qingqing</td><td>Postdoctoral Researcher, Department of Control Science and Engineering, Tongji University</td></tr>
        </tbody>
    </table>
</section>

<section id="team">
    <h2>Team Members</h2>
    <table>
        <thead>
            <tr><th>Name</th><th>Introduction</th></tr>
        </thead>
        <tbody>
            <tr><td>Yang Xu</td><td>Leader, Graduate student, Department of Control Science and Engineering, Tongji University</td></tr>
            <tr><td>Zhu Qijia</td><td>Graduate student, Department of Control Science and Engineering, Tongji University</td></tr>
            <tr><td>Qian Liang</td><td>Graduate student, Department of Control Science and Engineering, Tongji University</td></tr>
            <tr><td>Chen Yuzhe</td><td>Graduate student, Department of Control Science and Engineering, Tongji University</td></tr>
            <tr><td>Sheng Kai</td><td>Senior student, Artificial Intelligence, Tongji University</td></tr>
            <tr><td>Zhu Siya</td><td>Senior student, Mathematics, Tongji University</td></tr>
            <tr><td>Wang Xiangyi</td><td>Junior student, Artificial Intelligence, Tongji University</td></tr>
            <tr><td>Qi Chengkai</td><td>Junior student, Artificial Intelligence, Tongji University</td></tr>
            <tr><td>Li Yingxuan</td><td>Sophomore student, Robotics, Tongji University</td></tr>
        </tbody>
    </table>
</section>

<section id="history">
    <h2>RoboCup Participation</h2>
    <p>Previous participation and rankings in RoboCup and local tournaments (China Region).</p>
    <ul>
        <li>2019 RoboCup SSPL (China) – Champion</li>
        <li>2020 RoboCup SSPL (China) – Champion</li>
        <li>2021 RoboCup SSPL (China) – Champion</li>
        <li>2021 RoboCup OPL (China) – Champion</li>
        <li>2022 RoboCup SSPL (China) – First Prize</li>
        <li>2025 RoboCup OPL (China) – Champion</li>
    </ul>
    <p>Learn more about our lab and the competitions:</p>
    <ul>
        <li><a href="https://rail.tongji.edu.cn/" target="_blank">RAIL, Tongji University</a></li>
        <li><a href="http://www.robocup.org/" target="_blank">RoboCup Official Website</a></li>
        <li><a href="https://athome.robocup.org/" target="_blank">RoboCup@Home League</a></li>
    </ul>
</section>

<!-- Image Lightbox -->
<script>
document.querySelectorAll(".gallery img").forEach(img => {
    img.onclick = () => {
        const overlay = document.createElement("div");
        overlay.style = `
            position:fixed;
            top:0;left:0;
            width:100%;height:100%;
            background:rgba(0,0,0,0.85);
            display:flex;
            align-items:center;
            justify-content:center;
            z-index:9999;
        `;
        const bigImg = img.cloneNode();
        bigImg.style.maxWidth = "90%";
        bigImg.style.maxHeight = "90%";
        overlay.appendChild(bigImg);
        overlay.onclick = () => overlay.remove();
        document.body.appendChild(overlay);
    };
});
</script>

</body>
</html>
